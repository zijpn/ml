<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="description" content="Machine Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Machine Learning</title>

    <link rel="apple-touch-icon" href="apple-touch-icon.png">
    <link rel="icon" href="favicon.ico" type="image/x-icon">

    <!-- build:css styles/vendor.css -->
    <!-- bower:css -->
    <link rel="stylesheet" href="/bower_components/bootstrap/dist/css/bootstrap.css" />
    <!-- endbower -->
    <!-- endbuild -->

    <!-- build:css styles/main.css -->
    <link rel="stylesheet" href="styles/main.css">
    <!-- endbuild -->

  </head>
  <body>

    <nav class="navbar navbar-inverse">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class='icon-bar'></span>
            <span class='icon-bar'></span>
            <span class='icon-bar'></span>
          </button>
          <a class="navbar-brand" href="index.html">Machine Learning</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="linear.html">Linear Regression</a></li>
            <li><a href="logistic.html">Logistic Regression</a></li>
            <li class="active"><a href="neural.html">Neural Network</a></li>
            <li><a href="svm.html">Support Vector Machine</a></li>
          </ul>
        </div>
      </div>
    </nav>

    <div class="container-fixed">
      <table class="table">
        <tr><th>Hypothesis</th></tr>
        <tr>
          <td>$$ \begin{align*}
                 h_\Theta(x) &= a^{(L)} = g(z^{(L)}) \\
                 z^{(l)} &= \Theta^{(l-1)}a^{(l-1)} \\
                 a^{(1)} &= x
                 \end{align*}
              $$
              where
              $$ \begin{align*}
                 L &= \mbox{number of layers} \\
                 l &= \mbox{one layer in } [1..L] \\
                 g &= \mbox{sigmoid function} \\
                 a^{(l)} &= \mbox{activation vector of layer } l \\
                 \Theta^{(l)} &\in \mathbb{R}^{s_{(l+1)} \times (s_{l} + 1)}
                 \end{align*}
              $$
          </td>
        </tr>
        <tr><th>Cost Function</th></tr>
        <tr>
          <td>$$ \begin{align*}
                 J(\Theta) = & -\frac{1}{m}\left[ \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} log(h_\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\Theta(x^{i}))_k) \right] \\
                             & + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} (\Theta_{ji}^{(l)})^2
                  \end{align*}
              $$
              where
              $$ \begin{align*}
                 s_l &= \mbox{# of units (not counting bias unit) in layer } l \\
                 s_L &= \mbox{# of output units } = K \\
                 \Theta_{ji}^{(l)} &= \mbox{weight from layer } l \mbox{ mapping } a_{i}^{(l)} \mbox { to } a_{j}^{(l+1)}
                 \end{align*}
              $$
          </td>
        </tr>
        <tr><th>Example</th></tr>
        <tr>
          <td>
            <img src="images/nn.png" class="img-responsive center-block">
          </td>
        </tr>
        <tr><th>Algorithms</th></tr>
        <tr>
          <td>
            <ol>
              <li>Backpropagation
              $$ \begin{align*}
                 D_{ij}^{(l)} &:= \frac{1}{m} \Delta_{ij}^{(l)} & (j = 0)\\
                 D_{ij}^{(l)} &:= \frac{1}{m} \left[ \Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)} \right] & (j \neq 0) 
                 \end{align*}
              $$
              where
              $$ \begin{align*}
                 \Delta_{ij}^{(l)} &:= \Delta_{ij}^{(l)} + a_{j}^{(l)}\delta_{i}^{(l+1)} \\
                 \end{align*}
              $$
              and
              $$ \begin{align*}
                 \delta_{i}^{(l)} &= \mbox{'error' of node } i \mbox{ in layer } l \\
                 \delta^{(L)} &= a^{(L)} - y &\\
                 \delta^{(l)} &= ((\Theta^{(l)})^T \delta^{(l+1)}) .* a^{(l)}.*(1-a^{(l)})
                 \end{align*}
              $$
              </li>
            </ol>
          </td>
        </tr>
      </table>
    </div>

    <!-- build:js scripts/vendor.js -->
    <!-- bower:js -->
    <script src="/bower_components/jquery/dist/jquery.js"></script>
    <script src="/bower_components/bootstrap/dist/js/bootstrap.js"></script>
    <script src="/bower_components/d3/d3.js"></script>
    <!-- endbower -->
    <!-- endbuild -->

    <!-- build:js scripts/neural.js -->
    <script src="scripts/mathjax-config.js"></script>
    <!-- endbuild -->

    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>

  </body>
</html>
